{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a13ea4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Imports & MLflow Setup\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Determine MLflow tracking URI (env override -> SageMaker default -> local sqlite)\n",
    "tracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\")\n",
    "if not tracking_uri:\n",
    "    sage_db = Path(\"/home/ec2-user/SageMaker/ML-Ops-CreditCard-AWS/mlflow.db\")\n",
    "    if sage_db.exists():\n",
    "        tracking_uri = f\"sqlite:///{sage_db}\"\n",
    "    else:\n",
    "        local_db = Path.cwd() / \"mlflow.db\"\n",
    "        if local_db.exists():\n",
    "            tracking_uri = f\"sqlite:///{local_db}\"\n",
    "        else:\n",
    "            # Fallback: create local sqlite file and use it (this avoids needing a running mlflow server)\n",
    "            db_path = local_db\n",
    "            tracking_uri = f\"sqlite:///{db_path}\"\n",
    "            print(\"‚ö†Ô∏è No existing MLflow DB found. Using local sqlite at\", db_path)\n",
    "            print(\"If you intended to use a remote MLflow server, set the MLFLOW_TRACKING_URI environment variable or start an MLflow server.\")\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_registry_uri(mlflow.get_tracking_uri())\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c84c6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"creditcard-fraud-model\"  # must match name used in register_model.ipynb\n",
    "\n",
    "METRICS_TO_COMPARE = [\n",
    "    \"Accuracy\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1 Score\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee797c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Utility Functions (Registry & Metrics)\n",
    "\n",
    "def get_model_version_by_tag(client, model_name, tag_key, tag_value):\n",
    "    \"\"\"Return the MLflow Model Version object that matches a tag value, or None.\"\"\"\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    for v in versions:\n",
    "        try:\n",
    "            mv = client.get_model_version(name=model_name, version=v.version)\n",
    "            tags = mv.tags or {}\n",
    "            if tags.get(tag_key) == tag_value:\n",
    "                return mv\n",
    "        except Exception:\n",
    "            # ignore and continue\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_model_version_metrics(client, model_name, version):\n",
    "    mv = client.get_model_version(name=model_name, version=version)\n",
    "    run = client.get_run(mv.run_id)\n",
    "    return run.data.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d374dbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Metric Comparison Logic\n",
    "\n",
    "def better_than(challenger_metrics, champion_metrics):\n",
    "    \"\"\"Return True if challenger beats champion on a strict majority of compared metrics.\n",
    "    Only metrics present in both runs are considered; ties do not count for challenger.\n",
    "    \"\"\"\n",
    "    challenger_wins = 0\n",
    "    total_considered = 0\n",
    "\n",
    "    for metric in METRICS_TO_COMPARE:\n",
    "        c_val = challenger_metrics.get(metric)\n",
    "        champ_val = champion_metrics.get(metric)\n",
    "\n",
    "        if c_val is None or champ_val is None:\n",
    "            # missing metric in one of the runs: skip\n",
    "            continue\n",
    "\n",
    "        total_considered += 1\n",
    "        if c_val > champ_val:\n",
    "            challenger_wins += 1\n",
    "\n",
    "    if total_considered == 0:\n",
    "        # no comparable metrics; do not promote\n",
    "        return False\n",
    "\n",
    "    return challenger_wins > (total_considered / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261de332",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Champion Selection Logic\n",
    "\n",
    "def select_champion():\n",
    "    print(\"üöÄ Starting AWS MLflow Champion Selection\")\n",
    "\n",
    "    challenger = get_model_version_by_tag(client, MODEL_NAME, \"role\", \"challenger\")\n",
    "    if not challenger:\n",
    "        print(\"‚ùå No challenger model found\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚ÑπÔ∏è Challenger found ‚Üí version {challenger.version}\")\n",
    "\n",
    "    champion = get_model_version_by_tag(client, MODEL_NAME, \"role\", \"champion\")\n",
    "\n",
    "    # No champion exists\n",
    "    if not champion:\n",
    "        print(\"‚ö†Ô∏è No champion found ‚Äî promoting challenger directly\")\n",
    "\n",
    "        client.set_model_version_tag(MODEL_NAME, challenger.version, \"role\", \"champion\")\n",
    "        client.set_model_version_tag(MODEL_NAME, challenger.version, \"status\", \"production\")\n",
    "\n",
    "        print(f\"‚úÖ Challenger v{challenger.version} promoted to Champion\")\n",
    "        return\n",
    "\n",
    "    print(f\"‚ÑπÔ∏è Champion found ‚Üí version {champion.version}\")\n",
    "\n",
    "    challenger_metrics = get_model_version_metrics(client, MODEL_NAME, challenger.version)\n",
    "    champion_metrics = get_model_version_metrics(client, MODEL_NAME, champion.version)\n",
    "\n",
    "    print(\"\\nüìä Metrics Comparison\")\n",
    "    print(f\"{'Metric':<25}{'Challenger':<15}{'Champion':<15}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for metric in METRICS_TO_COMPARE:\n",
    "        print(\n",
    "            f\"{metric:<25}\"\n",
    "            f\"{str(challenger_metrics.get(metric, 'N/A')):<15}\"\n",
    "            f\"{str(champion_metrics.get(metric, 'N/A')):<15}\"\n",
    "        )\n",
    "\n",
    "    if better_than(challenger_metrics, champion_metrics):\n",
    "        print(\"\\nüöÄ Challenger outperforms Champion ‚Üí Promoting\")\n",
    "\n",
    "        # Archive old champion\n",
    "        client.set_model_version_tag(MODEL_NAME, champion.version, \"role\", \"archived\")\n",
    "        client.set_model_version_tag(MODEL_NAME, champion.version, \"status\", \"archived\")\n",
    "\n",
    "        # Promote challenger\n",
    "        client.set_model_version_tag(MODEL_NAME, challenger.version, \"role\", \"champion\")\n",
    "        client.set_model_version_tag(MODEL_NAME, challenger.version, \"status\", \"production\")\n",
    "\n",
    "        print(f\"‚úÖ Challenger v{challenger.version} is now Champion\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Challenger did NOT outperform Champion ‚Äî no change\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52657afc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    select_champion()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
